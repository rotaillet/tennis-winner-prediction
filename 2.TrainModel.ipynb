{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT**: \"./data/1finalDataset.csv\"\n",
    "\n",
    "**OUTPUT**: Outputs the XGBoostModels \"./models/best_xgb_model.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take the final dataset (which contains all the tennis statistics), and we train several models with it (Random Forest, XGBoost, Neural Net). Then, we will save the best models to the models folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def seed_everything(seed: int = 41):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # si multi-GPU\n",
    "\n",
    "    # Pour forcer la reproductibilité sur CUDA (moins perf mais stable)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Exemple :\n",
    "seed_everything(41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE_DIFF', 'ATP_POINTS_DIFF', 'ATP_RANK_DIFF', 'BEST_OF', 'DRAW_SIZE',\n",
       "       'ELO_DIFF', 'ELO_GRAD_LAST_100_DIFF', 'ELO_GRAD_LAST_10_DIFF',\n",
       "       'ELO_GRAD_LAST_200_DIFF', 'ELO_GRAD_LAST_25_DIFF',\n",
       "       'ELO_GRAD_LAST_3_DIFF', 'ELO_GRAD_LAST_50_DIFF', 'ELO_GRAD_LAST_5_DIFF',\n",
       "       'ELO_SURFACE_DIFF', 'H2H_DIFF', 'H2H_SURFACE_DIFF', 'HEIGHT_DIFF',\n",
       "       'N_GAMES_DIFF', 'P_1ST_IN_LAST_100_DIFF', 'P_1ST_IN_LAST_10_DIFF',\n",
       "       'P_1ST_IN_LAST_200_DIFF', 'P_1ST_IN_LAST_25_DIFF',\n",
       "       'P_1ST_IN_LAST_3_DIFF', 'P_1ST_IN_LAST_50_DIFF', 'P_1ST_IN_LAST_5_DIFF',\n",
       "       'P_1ST_WON_LAST_100_DIFF', 'P_1ST_WON_LAST_10_DIFF',\n",
       "       'P_1ST_WON_LAST_200_DIFF', 'P_1ST_WON_LAST_25_DIFF',\n",
       "       'P_1ST_WON_LAST_3_DIFF', 'P_1ST_WON_LAST_50_DIFF',\n",
       "       'P_1ST_WON_LAST_5_DIFF', 'P_2ND_WON_LAST_100_DIFF',\n",
       "       'P_2ND_WON_LAST_10_DIFF', 'P_2ND_WON_LAST_200_DIFF',\n",
       "       'P_2ND_WON_LAST_25_DIFF', 'P_2ND_WON_LAST_3_DIFF',\n",
       "       'P_2ND_WON_LAST_50_DIFF', 'P_2ND_WON_LAST_5_DIFF',\n",
       "       'P_ACE_LAST_100_DIFF', 'P_ACE_LAST_10_DIFF', 'P_ACE_LAST_200_DIFF',\n",
       "       'P_ACE_LAST_25_DIFF', 'P_ACE_LAST_3_DIFF', 'P_ACE_LAST_50_DIFF',\n",
       "       'P_ACE_LAST_5_DIFF', 'P_BP_SAVED_LAST_100_DIFF',\n",
       "       'P_BP_SAVED_LAST_10_DIFF', 'P_BP_SAVED_LAST_200_DIFF',\n",
       "       'P_BP_SAVED_LAST_25_DIFF', 'P_BP_SAVED_LAST_3_DIFF',\n",
       "       'P_BP_SAVED_LAST_50_DIFF', 'P_BP_SAVED_LAST_5_DIFF',\n",
       "       'P_DF_LAST_100_DIFF', 'P_DF_LAST_10_DIFF', 'P_DF_LAST_200_DIFF',\n",
       "       'P_DF_LAST_25_DIFF', 'P_DF_LAST_3_DIFF', 'P_DF_LAST_50_DIFF',\n",
       "       'P_DF_LAST_5_DIFF', 'TOURNEY_LEVEL', 'WIN_LAST_100_DIFF',\n",
       "       'WIN_LAST_10_DIFF', 'WIN_LAST_200_DIFF', 'WIN_LAST_25_DIFF',\n",
       "       'WIN_LAST_3_DIFF', 'WIN_LAST_50_DIFF', 'WIN_LAST_5_DIFF', 'date',\n",
       "       'p1_elo', 'p2_elo', 'p1_id', 'p2_id', 'RESULT', 'SCORE', 'score_n_sets',\n",
       "       'score_sets_src', 'score_sets_dst', 'score_games_src_total',\n",
       "       'score_games_dst_total', 'score_games_diff_total', 'score_sets_diff',\n",
       "       'score_bagels_src', 'score_bagels_dst', 'score_tiebreaks',\n",
       "       'score_super_tb_flag', 'score_super_tb_src_pts',\n",
       "       'score_super_tb_dst_pts', 'score_straight_sets', 'score_closeness',\n",
       "       'score_valid', 'date2', 't_days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = pd.read_csv(\"./data/1finalDataset.csv\")\n",
    "final_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de joueurs: 1932\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Récupère toutes les valeurs uniques\n",
    "all_ids = pd.unique(final_dataset[[\"p1_id\", \"p2_id\"]].values.ravel())\n",
    "\n",
    "# Crée un mapping {id_original -> id_compact}\n",
    "id_map = {old_id: new_id for new_id, old_id in enumerate(all_ids)}\n",
    "\n",
    "# Applique le mapping\n",
    "final_dataset[\"p1_id\"] = final_dataset[\"p1_id\"].map(id_map)\n",
    "final_dataset[\"p2_id\"] = final_dataset[\"p2_id\"].map(id_map)\n",
    "\n",
    "# Nombre de noeuds réels\n",
    "num_nodes = len(all_ids)\n",
    "print(\"Nombre de joueurs:\", num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92429\n",
      "2946\n"
     ]
    }
   ],
   "source": [
    "date = 20240101\n",
    "train_df = final_dataset[final_dataset[\"date\"] < date].copy()\n",
    "test_df  = final_dataset[final_dataset[\"date\"] >= date].copy()\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = [\n",
    "    'AGE_DIFF', 'ATP_POINTS_DIFF', 'ATP_RANK_DIFF', 'BEST_OF', 'DRAW_SIZE',\n",
    "       'ELO_DIFF', 'ELO_GRAD_LAST_100_DIFF', 'ELO_GRAD_LAST_10_DIFF',\n",
    "       'ELO_GRAD_LAST_200_DIFF', 'ELO_GRAD_LAST_25_DIFF',\n",
    "       'ELO_GRAD_LAST_3_DIFF', 'ELO_GRAD_LAST_50_DIFF', 'ELO_GRAD_LAST_5_DIFF',\n",
    "       'ELO_SURFACE_DIFF', 'H2H_DIFF', 'H2H_SURFACE_DIFF', 'HEIGHT_DIFF',\n",
    "       'N_GAMES_DIFF', 'P_1ST_IN_LAST_100_DIFF', 'P_1ST_IN_LAST_10_DIFF',\n",
    "       'P_1ST_IN_LAST_200_DIFF', 'P_1ST_IN_LAST_25_DIFF',\n",
    "       'P_1ST_IN_LAST_3_DIFF', 'P_1ST_IN_LAST_50_DIFF', 'P_1ST_IN_LAST_5_DIFF',\n",
    "       'P_1ST_WON_LAST_100_DIFF', 'P_1ST_WON_LAST_10_DIFF',\n",
    "       'P_1ST_WON_LAST_200_DIFF', 'P_1ST_WON_LAST_25_DIFF',\n",
    "       'P_1ST_WON_LAST_3_DIFF', 'P_1ST_WON_LAST_50_DIFF',\n",
    "       'P_1ST_WON_LAST_5_DIFF', 'P_2ND_WON_LAST_100_DIFF',\n",
    "       'P_2ND_WON_LAST_10_DIFF', 'P_2ND_WON_LAST_200_DIFF',\n",
    "       'P_2ND_WON_LAST_25_DIFF', 'P_2ND_WON_LAST_3_DIFF',\n",
    "       'P_2ND_WON_LAST_50_DIFF', 'P_2ND_WON_LAST_5_DIFF',\n",
    "       'P_ACE_LAST_100_DIFF', 'P_ACE_LAST_10_DIFF', 'P_ACE_LAST_200_DIFF',\n",
    "       'P_ACE_LAST_25_DIFF', 'P_ACE_LAST_3_DIFF', 'P_ACE_LAST_50_DIFF',\n",
    "       'P_ACE_LAST_5_DIFF', 'P_BP_SAVED_LAST_100_DIFF',\n",
    "       'P_BP_SAVED_LAST_10_DIFF', 'P_BP_SAVED_LAST_200_DIFF',\n",
    "       'P_BP_SAVED_LAST_25_DIFF', 'P_BP_SAVED_LAST_3_DIFF',\n",
    "       'P_BP_SAVED_LAST_50_DIFF', 'P_BP_SAVED_LAST_5_DIFF',\n",
    "       'P_DF_LAST_100_DIFF', 'P_DF_LAST_10_DIFF', 'P_DF_LAST_200_DIFF',\n",
    "       'P_DF_LAST_25_DIFF', 'P_DF_LAST_3_DIFF', 'P_DF_LAST_50_DIFF',\n",
    "       'P_DF_LAST_5_DIFF', 'WIN_LAST_100_DIFF',\n",
    "       'WIN_LAST_10_DIFF', 'WIN_LAST_200_DIFF', 'WIN_LAST_25_DIFF',\n",
    "       'WIN_LAST_3_DIFF', 'WIN_LAST_50_DIFF', 'WIN_LAST_5_DIFF'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training vs Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll shuffle the data, and do a 85% split between training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "def build_temporal_data(df_subset):\n",
    "    return TemporalData(\n",
    "        src = torch.tensor(df_subset[\"p1_id\"].values, dtype=torch.long),\n",
    "        dst = torch.tensor(df_subset[\"p2_id\"].values, dtype=torch.long),\n",
    "        t   = torch.tensor(df_subset[\"t_days\"].values, dtype=torch.long),\n",
    "        msg = torch.tensor(df_subset[features_cols].values, dtype=torch.float),\n",
    "        y   = torch.tensor(df_subset[\"RESULT\"].values, dtype=torch.float),\n",
    "        closeness = torch.tensor(df_subset[\"score_closeness\"].values,dtype=torch.float)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the result column to string values (since that's what the sklearn library requires I'm pretty sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = build_temporal_data(final_dataset)\n",
    "train_data = build_temporal_data(train_df)\n",
    "test_data  = build_temporal_data(test_df)\n",
    "\n",
    "for data in (full_data,train_data, test_data):\n",
    "    data.src = data.src.long()    # src en ints longs\n",
    "    data.dst = data.dst.long()    # dst en ints longs\n",
    "    data.t   = data.t.long()     # timestamps en floats\n",
    "    data.msg = data.msg.float()   # features en floats\n",
    "    data.y   = data.y.float()     # labels en floats\n",
    "    data.closeness = data.closeness.float()\n",
    "# 4) Déplacer t et msg sur GPU\n",
    "device = \"cuda\"\n",
    "for data in (full_data,train_data, test_data):\n",
    "    data.t   = data.t.to(device)\n",
    "    data.msg = data.msg.to(device)\n",
    "# 6. DataLoaders pour entraînement\n",
    "train_loader = TemporalDataLoader(train_data, batch_size=32, neg_sampling_ratio=0)\n",
    "test_loader  = TemporalDataLoader(test_data, batch_size=32, neg_sampling_ratio=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nouveau batch ===\n",
      "src: tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,  2,  6,\n",
      "        10, 12, 19, 22, 24, 29,  0, 27,  5, 22,  5, 32, 34, 36])\n",
      "dst: tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,  0,  5,\n",
      "         8, 15, 16, 21, 27, 30,  5, 29, 15, 29, 22, 33, 35, 37])\n",
      "t: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "msg: tensor([[ 0.9601,  0.5384, -0.5188,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-1.9789,  0.1350, -0.9501,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-0.7111, -0.1837,  1.0249,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        ...,\n",
      "        [ 0.6143,  1.0605, -0.3902,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-0.5575,  0.1548, -0.1783,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-1.0377, -0.1670,  0.3741,  ..., -0.0022,  0.0043, -0.0023]],\n",
      "       device='cuda:0')\n",
      "y: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "Closeness: tensor([0.5882, 0.5882, 0.7826, 1.0000, 0.7368, 0.4000, 0.7619, 0.8966, 0.5882,\n",
      "        0.8462, 0.9655, 0.9333, 0.4000, 0.6667, 0.5000, 0.5882, 1.0000, 0.5000,\n",
      "        0.9375, 0.5000, 0.9697, 0.8000, 0.9375, 0.8387, 0.7619, 0.8966, 0.9286,\n",
      "        0.7692, 0.9231, 0.8571, 0.5000, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_loader:\n",
    "    print(\"=== Nouveau batch ===\")\n",
    "    print(\"src:\", batch.src)\n",
    "    print(\"dst:\", batch.dst)\n",
    "    print(\"t:\", batch.t)\n",
    "    print(\"msg:\", batch.msg)\n",
    "    print(\"y:\", batch.y)\n",
    "    print(\"Closeness:\", batch.closeness)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class History:\n",
    "    def __init__(self, outdir=\"runs/curves\", hparams=None):\n",
    "        \"\"\"\n",
    "        hparams: dict optionnel avec les hyperparamètres \n",
    "                 (ex: {\"lr\":4e-4,\"weight_decay\":1e-5,\"memory_dim\":128,...})\n",
    "        \"\"\"\n",
    "        self.outdir = Path(outdir)\n",
    "        self.outdir.mkdir(parents=True, exist_ok=True)\n",
    "        self.rows = []\n",
    "        self.prec_at_rows = []\n",
    "        self.detail_rows = []\n",
    "        self.hparams = hparams if hparams is not None else {}\n",
    "\n",
    "    def log_epoch(self, epoch,\n",
    "                  train_loss, train_ap, train_prec,\n",
    "                  val_loss,   val_ap,   val_prec,\n",
    "                  prec_at=None):\n",
    "        self.rows.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"train_ap\": float(train_ap),\n",
    "            \"train_prec@0.5\": float(train_prec),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_ap\": float(val_ap),\n",
    "            \"val_prec@0.5\": float(val_prec),\n",
    "        })\n",
    "        if prec_at is not None:\n",
    "            self.prec_at_rows.append(\n",
    "                {\"epoch\": epoch, **{f\"@{k}\": float(v) for k, v in prec_at.items()}}\n",
    "            )\n",
    "\n",
    "    def save_tables(self):\n",
    "        pd.DataFrame(self.rows).to_csv(self.outdir / \"metrics_history.csv\", index=False)\n",
    "        if self.prec_at_rows:\n",
    "            pd.DataFrame(self.prec_at_rows).to_csv(self.outdir / \"precision_at_history.csv\", index=False)\n",
    "        if self.detail_rows:\n",
    "            pd.DataFrame(self.detail_rows).to_csv(self.outdir / \"predicted_dates.csv\", index=False)\n",
    "        # 💾 Sauvegarde aussi les hyperparamètres dans un JSON\n",
    "        if self.hparams:\n",
    "            with open(self.outdir / \"hparams.json\",\"w\") as f:\n",
    "                json.dump(self.hparams,f,indent=2)\n",
    "\n",
    "    def _plot_and_save(self, x, y, ylabel, fname):\n",
    "        plt.figure()\n",
    "        plt.plot(x, y)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.outdir / fname, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    def save_plots(self):\n",
    "        df = pd.DataFrame(self.rows)\n",
    "        x = df[\"epoch\"].values\n",
    "        self._plot_and_save(x, df[\"train_loss\"].values, \"train_loss\", \"curve_train_loss.png\")\n",
    "        self._plot_and_save(x, df[\"val_loss\"].values,   \"val_loss\",   \"curve_val_loss.png\")\n",
    "        self._plot_and_save(x, df[\"train_ap\"].values,   \"train_AP\",   \"curve_train_ap.png\")\n",
    "        self._plot_and_save(x, df[\"val_ap\"].values,     \"val_AP\",     \"curve_val_ap.png\")\n",
    "        self._plot_and_save(x, df[\"train_prec@0.5\"].values, \"train_precision@0.5\", \"curve_train_prec.png\")\n",
    "        self._plot_and_save(x, df[\"val_prec@0.5\"].values,   \"val_precision@0.5\",   \"curve_val_prec.png\")\n",
    "\n",
    "    def save_all(self):\n",
    "        self.save_tables()\n",
    "        self.save_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN 1 | lr=0.001 | wd=0.0001 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:05<00:00, 43.93batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 104.57batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:06<00:00, 43.36batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 92.23batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:07<00:00, 42.57batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 102.38batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:11<00:00, 40.20batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 96.13batch/s] \n",
      "Training: 100%|██████████| 2889/2889 [01:10<00:00, 40.77batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 100.72batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:13<00:00, 39.38batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.66batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:15<00:00, 38.21batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.16batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.60batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.57batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.87batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.88batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.38batch/s] \n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 85.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run1_lr0.001_wd0.0001\n",
      "\n",
      "=== RUN 2 | lr=0.001 | wd=0.0001 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:13<00:00, 39.14batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 94.59batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:15<00:00, 38.15batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:03<00:00, 25.52batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:14<00:00, 38.61batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.96batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:14<00:00, 38.61batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 91.47batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:13<00:00, 39.36batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 97.05batch/s] \n",
      "Training: 100%|██████████| 2889/2889 [01:13<00:00, 39.34batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.21batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 35.71batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 66.93batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.84batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.31batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:14<00:00, 38.94batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 73.04batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.26batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 81.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run2_lr0.001_wd0.0001\n",
      "\n",
      "=== RUN 3 | lr=0.001 | wd=0.0005 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:22<00:00, 34.92batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.47batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.23batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 74.35batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.70batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.55batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 35.94batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 81.62batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.35batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.14batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:15<00:00, 38.29batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.21batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.90batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 83.31batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.22batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.63batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.07batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.37batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run3_lr0.001_wd0.0005\n",
      "\n",
      "=== RUN 4 | lr=0.001 | wd=0.0005 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 36.10batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.02batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.79batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.50batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.46batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.52batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.12batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.94batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.08batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.60batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.50batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 93.90batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.88batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.37batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 35.98batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.83batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.75batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 89.27batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.55batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run4_lr0.001_wd0.0005\n",
      "\n",
      "=== RUN 5 | lr=0.0004 | wd=0.0001 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.43batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.95batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.26batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.36batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.13batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 93.26batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:14<00:00, 38.58batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.20batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:13<00:00, 39.38batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:00<00:00, 95.79batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.21batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.65batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:23<00:00, 34.74batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.77batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.42batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.92batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.48batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 81.24batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 36.02batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 86.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run5_lr0.0004_wd0.0001\n",
      "\n",
      "=== RUN 6 | lr=0.0004 | wd=0.0001 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.07batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.80batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 85.89batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:04<00:00, 22.29batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.18batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 86.89batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.75batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 86.00batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.31batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.20batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:22<00:00, 34.87batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 88.37batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.83batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 87.46batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:18<00:00, 36.81batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.69batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:22<00:00, 34.86batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run6_lr0.0004_wd0.0001\n",
      "\n",
      "=== RUN 7 | lr=0.0004 | wd=0.0005 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.17batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 83.43batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:23<00:00, 34.70batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 86.95batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.51batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.56batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:20<00:00, 35.68batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 84.87batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:24<00:00, 34.21batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 83.60batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:21<00:00, 35.35batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 85.03batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:19<00:00, 36.42batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 82.42batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:21<00:00, 35.37batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 82.40batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:17<00:00, 37.12batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 75.92batch/s]\n",
      "Training: 100%|██████████| 2889/2889 [01:22<00:00, 35.18batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 83.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping (aucune amélioration après 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegardés dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run7_lr0.0004_wd0.0005\n",
      "\n",
      "=== RUN 8 | lr=0.0004 | wd=0.0005 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2889/2889 [01:16<00:00, 37.93batch/s]\n",
      "Evaluating: 100%|██████████| 93/93 [00:01<00:00, 90.50batch/s]\n",
      "Training:  91%|█████████ | 2616/2889 [01:10<00:07, 35.33batch/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from torch_geometric.nn import TGNMemory\n",
    "from torch_geometric.nn.models.tgn import (\n",
    "    LastAggregator,\n",
    "    LastNeighborLoader,\n",
    "    IdentityMessage\n",
    ")\n",
    "from tgn.model import MultiLayerTimeAwareGNN,MessageMLP,WinPredictorMLP,WinPredictor,SmallWinPredictor\n",
    "from tgn.utils import train,evaluate,compute_alpha,train_debug,train_debug2\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Paramètres\n",
    "memory_dim = 128\n",
    "time_dim   = 32\n",
    "embedding_dim = 128\n",
    "in_channels = 128\n",
    "hidden_channels = 32\n",
    "\n",
    "num_layers = 2\n",
    "heads = 4\n",
    "dropout= 0.4\n",
    "learning_rates = [1e-3, 4e-4]\n",
    "weight_decays = [1e-4, 5e-4]\n",
    "hidden_variants = [[512, 64], [256, 64]]\n",
    "\n",
    "# Générer toutes les combinaisons\n",
    "grid = list(product(learning_rates, weight_decays, hidden_variants))\n",
    "run_id = 0\n",
    "for lr, wd, hidden in grid:\n",
    "    run_id += 1\n",
    "    print(f\"\\n=== RUN {run_id} | lr={lr} | wd={wd} | hidden={hidden} ===\")\n",
    "\n",
    "    msg_dim = full_data.msg.size(-1)\n",
    "\n",
    "    memory = TGNMemory(\n",
    "        num_nodes=num_nodes,\n",
    "        raw_msg_dim=msg_dim,\n",
    "        memory_dim=memory_dim,\n",
    "        time_dim=time_dim,\n",
    "        message_module=MessageMLP(msg_dim, memory_dim, time_dim,2*memory_dim),\n",
    "        aggregator_module=LastAggregator(),\n",
    "    ).to(device)\n",
    "\n",
    "    gnn = MultiLayerTimeAwareGNN(in_channels,memory_dim,hidden_channels, \n",
    "                                 embedding_dim, msg_dim, memory.time_enc,\n",
    "                                 num_layers,heads,dropout).to(device)\n",
    "    \n",
    "    win_pred = SmallWinPredictor(\n",
    "        embed_dim=embedding_dim,\n",
    "        match_dim=msg_dim,\n",
    "        hidden = hidden \n",
    "    ).to(device)\n",
    "\n",
    "    total_params = 0\n",
    "    for model in [memory, gnn, win_pred]:\n",
    "        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"{model.__class__.__name__} params: {model_params:,}\")\n",
    "        total_params += model_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(memory.parameters()) + list(gnn.parameters()) + list(win_pred.parameters()),\n",
    "        lr=lr,weight_decay=wd\n",
    "    )\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # === Loaders ===\n",
    "\n",
    "\n",
    "    train_loader_ngh = LastNeighborLoader(num_nodes=num_nodes, size=25, device=device)\n",
    "    eval_loader_ngh  = LastNeighborLoader(num_nodes=num_nodes, size=25, device=device)\n",
    "\n",
    "    assoc = torch.empty(num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\n",
    "    threshold = [0.6,0.65,0.7,0.75,0.8]\n",
    "    num_epochs = 200\n",
    "\n",
    "    import random\n",
    "\n",
    "    train_variants = [\n",
    "        (train_loader, full_data, train_data),\n",
    "\n",
    "    ]\n",
    "    patience = 10         # nombre d'epochs sans amélioration avant arrêt\n",
    "    min_delta = 1e-4      # amélioration minimale pour reset la patience\n",
    "    best_val_loss = 100\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.9)\n",
    "    hparams = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": wd,\n",
    "    \"memory_dim\": memory_dim,\n",
    "    \"time_dim\": time_dim,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "    \"hidden\": hidden\n",
    "    }\n",
    "\n",
    "    run_dir = f\"runs/run{run_id}_lr{lr}_wd{wd}\"\n",
    "    os.makedirs(run_dir.replace(\"runs/\", \"models/\"), exist_ok=True)\n",
    "    history = History(outdir=run_dir,hparams=hparams)\n",
    "\n",
    "    train_losses, train_aps, train_prec = [], [], []\n",
    "    val_losses,   val_aps,   val_prec  = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        alpha = compute_alpha(epoch, num_epochs)\n",
    "\n",
    "        loader, full, train_data_split = random.choice(train_variants)\n",
    "\n",
    "        loss, ap, prec = train_debug2(\n",
    "            loader, memory, gnn, win_pred, full, train_loader_ngh, eval_loader_ngh,\n",
    "            optimizer, device, assoc, train_data_split, alpha\n",
    "        )\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        train_aps.append(ap)\n",
    "        train_prec.append(prec)\n",
    "\n",
    "        val_ap, val_loss, prec_v, prec_at, well_dates, bad_dates = evaluate(\n",
    "            test_loader, memory, gnn, win_pred, full_data, eval_loader_ngh,\n",
    "            assoc, device, threshold, alpha\n",
    "        )\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_aps.append(val_ap)\n",
    "        val_prec.append(prec_v)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"⏹️ Early stopping (aucune amélioration après {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "        # --- LOG + SAUVEGARDE INCRÉMENTALE  ---\n",
    "        history.log_epoch(\n",
    "            epoch=epoch,\n",
    "            train_loss=loss, train_ap=ap, train_prec=prec,\n",
    "            val_loss=val_loss, val_ap=val_ap, val_prec=prec_v,\n",
    "            prec_at=prec_at\n",
    "        )\n",
    "        \n",
    "        history.save_tables()\n",
    "        history.save_plots()\n",
    "        torch.save({\n",
    "        \"memory_state\": memory.state_dict(),\n",
    "        \"gnn_state\": gnn.state_dict(),\n",
    "        \"win_pred_state\": win_pred.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict()\n",
    "    }, f\"models/run{run_id}_lr{lr}_wd{wd}/epoch{epoch}.pth\")\n",
    "\n",
    "    \n",
    "    history.save_all()\n",
    "    print(\"Courbes et CSV sauvegardés dans\", history.outdir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
