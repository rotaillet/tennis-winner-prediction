{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT**: \"./data/1finalDataset.csv\"\n",
    "\n",
    "**OUTPUT**: Outputs the XGBoostModels \"./models/best_xgb_model.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take the final dataset (which contains all the tennis statistics), and we train several models with it (Random Forest, XGBoost, Neural Net). Then, we will save the best models to the models folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def seed_everything(seed: int = 41):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # si multi-GPU\n",
    "\n",
    "    # Pour forcer la reproductibilit√© sur CUDA (moins perf mais stable)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Exemple :\n",
    "seed_everything(41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE_DIFF', 'ATP_POINTS_DIFF', 'ATP_RANK_DIFF', 'BEST_OF', 'DRAW_SIZE',\n",
       "       'ELO_DIFF', 'ELO_GRAD_LAST_100_DIFF', 'ELO_GRAD_LAST_10_DIFF',\n",
       "       'ELO_GRAD_LAST_200_DIFF', 'ELO_GRAD_LAST_25_DIFF',\n",
       "       'ELO_GRAD_LAST_3_DIFF', 'ELO_GRAD_LAST_50_DIFF', 'ELO_GRAD_LAST_5_DIFF',\n",
       "       'ELO_SURFACE_DIFF', 'H2H_DIFF', 'H2H_SURFACE_DIFF', 'HEIGHT_DIFF',\n",
       "       'N_GAMES_DIFF', 'P_1ST_IN_LAST_100_DIFF', 'P_1ST_IN_LAST_10_DIFF',\n",
       "       'P_1ST_IN_LAST_200_DIFF', 'P_1ST_IN_LAST_25_DIFF',\n",
       "       'P_1ST_IN_LAST_3_DIFF', 'P_1ST_IN_LAST_50_DIFF', 'P_1ST_IN_LAST_5_DIFF',\n",
       "       'P_1ST_WON_LAST_100_DIFF', 'P_1ST_WON_LAST_10_DIFF',\n",
       "       'P_1ST_WON_LAST_200_DIFF', 'P_1ST_WON_LAST_25_DIFF',\n",
       "       'P_1ST_WON_LAST_3_DIFF', 'P_1ST_WON_LAST_50_DIFF',\n",
       "       'P_1ST_WON_LAST_5_DIFF', 'P_2ND_WON_LAST_100_DIFF',\n",
       "       'P_2ND_WON_LAST_10_DIFF', 'P_2ND_WON_LAST_200_DIFF',\n",
       "       'P_2ND_WON_LAST_25_DIFF', 'P_2ND_WON_LAST_3_DIFF',\n",
       "       'P_2ND_WON_LAST_50_DIFF', 'P_2ND_WON_LAST_5_DIFF',\n",
       "       'P_ACE_LAST_100_DIFF', 'P_ACE_LAST_10_DIFF', 'P_ACE_LAST_200_DIFF',\n",
       "       'P_ACE_LAST_25_DIFF', 'P_ACE_LAST_3_DIFF', 'P_ACE_LAST_50_DIFF',\n",
       "       'P_ACE_LAST_5_DIFF', 'P_BP_SAVED_LAST_100_DIFF',\n",
       "       'P_BP_SAVED_LAST_10_DIFF', 'P_BP_SAVED_LAST_200_DIFF',\n",
       "       'P_BP_SAVED_LAST_25_DIFF', 'P_BP_SAVED_LAST_3_DIFF',\n",
       "       'P_BP_SAVED_LAST_50_DIFF', 'P_BP_SAVED_LAST_5_DIFF',\n",
       "       'P_DF_LAST_100_DIFF', 'P_DF_LAST_10_DIFF', 'P_DF_LAST_200_DIFF',\n",
       "       'P_DF_LAST_25_DIFF', 'P_DF_LAST_3_DIFF', 'P_DF_LAST_50_DIFF',\n",
       "       'P_DF_LAST_5_DIFF', 'TOURNEY_LEVEL', 'WIN_LAST_100_DIFF',\n",
       "       'WIN_LAST_10_DIFF', 'WIN_LAST_200_DIFF', 'WIN_LAST_25_DIFF',\n",
       "       'WIN_LAST_3_DIFF', 'WIN_LAST_50_DIFF', 'WIN_LAST_5_DIFF', 'date',\n",
       "       'p1_elo', 'p2_elo', 'p1_id', 'p2_id', 'RESULT', 'SCORE', 'score_n_sets',\n",
       "       'score_sets_src', 'score_sets_dst', 'score_games_src_total',\n",
       "       'score_games_dst_total', 'score_games_diff_total', 'score_sets_diff',\n",
       "       'score_bagels_src', 'score_bagels_dst', 'score_tiebreaks',\n",
       "       'score_super_tb_flag', 'score_super_tb_src_pts',\n",
       "       'score_super_tb_dst_pts', 'score_straight_sets', 'score_closeness',\n",
       "       'score_valid', 'date2', 't_days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = pd.read_csv(\"./data/1finalDataset.csv\")\n",
    "final_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de joueurs: 1932\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# R√©cup√®re toutes les valeurs uniques\n",
    "all_ids = pd.unique(final_dataset[[\"p1_id\", \"p2_id\"]].values.ravel())\n",
    "\n",
    "# Cr√©e un mapping {id_original -> id_compact}\n",
    "id_map = {old_id: new_id for new_id, old_id in enumerate(all_ids)}\n",
    "\n",
    "# Applique le mapping\n",
    "final_dataset[\"p1_id\"] = final_dataset[\"p1_id\"].map(id_map)\n",
    "final_dataset[\"p2_id\"] = final_dataset[\"p2_id\"].map(id_map)\n",
    "\n",
    "# Nombre de noeuds r√©els\n",
    "num_nodes = len(all_ids)\n",
    "print(\"Nombre de joueurs:\", num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92429\n",
      "2946\n"
     ]
    }
   ],
   "source": [
    "date = 20240101\n",
    "train_df = final_dataset[final_dataset[\"date\"] < date].copy()\n",
    "test_df  = final_dataset[final_dataset[\"date\"] >= date].copy()\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = [\n",
    "    'AGE_DIFF', 'ATP_POINTS_DIFF', 'ATP_RANK_DIFF', 'BEST_OF', 'DRAW_SIZE',\n",
    "       'ELO_DIFF', 'ELO_GRAD_LAST_100_DIFF', 'ELO_GRAD_LAST_10_DIFF',\n",
    "       'ELO_GRAD_LAST_200_DIFF', 'ELO_GRAD_LAST_25_DIFF',\n",
    "       'ELO_GRAD_LAST_3_DIFF', 'ELO_GRAD_LAST_50_DIFF', 'ELO_GRAD_LAST_5_DIFF',\n",
    "       'ELO_SURFACE_DIFF', 'H2H_DIFF', 'H2H_SURFACE_DIFF', 'HEIGHT_DIFF',\n",
    "       'N_GAMES_DIFF', 'P_1ST_IN_LAST_100_DIFF', 'P_1ST_IN_LAST_10_DIFF',\n",
    "       'P_1ST_IN_LAST_200_DIFF', 'P_1ST_IN_LAST_25_DIFF',\n",
    "       'P_1ST_IN_LAST_3_DIFF', 'P_1ST_IN_LAST_50_DIFF', 'P_1ST_IN_LAST_5_DIFF',\n",
    "       'P_1ST_WON_LAST_100_DIFF', 'P_1ST_WON_LAST_10_DIFF',\n",
    "       'P_1ST_WON_LAST_200_DIFF', 'P_1ST_WON_LAST_25_DIFF',\n",
    "       'P_1ST_WON_LAST_3_DIFF', 'P_1ST_WON_LAST_50_DIFF',\n",
    "       'P_1ST_WON_LAST_5_DIFF', 'P_2ND_WON_LAST_100_DIFF',\n",
    "       'P_2ND_WON_LAST_10_DIFF', 'P_2ND_WON_LAST_200_DIFF',\n",
    "       'P_2ND_WON_LAST_25_DIFF', 'P_2ND_WON_LAST_3_DIFF',\n",
    "       'P_2ND_WON_LAST_50_DIFF', 'P_2ND_WON_LAST_5_DIFF',\n",
    "       'P_ACE_LAST_100_DIFF', 'P_ACE_LAST_10_DIFF', 'P_ACE_LAST_200_DIFF',\n",
    "       'P_ACE_LAST_25_DIFF', 'P_ACE_LAST_3_DIFF', 'P_ACE_LAST_50_DIFF',\n",
    "       'P_ACE_LAST_5_DIFF', 'P_BP_SAVED_LAST_100_DIFF',\n",
    "       'P_BP_SAVED_LAST_10_DIFF', 'P_BP_SAVED_LAST_200_DIFF',\n",
    "       'P_BP_SAVED_LAST_25_DIFF', 'P_BP_SAVED_LAST_3_DIFF',\n",
    "       'P_BP_SAVED_LAST_50_DIFF', 'P_BP_SAVED_LAST_5_DIFF',\n",
    "       'P_DF_LAST_100_DIFF', 'P_DF_LAST_10_DIFF', 'P_DF_LAST_200_DIFF',\n",
    "       'P_DF_LAST_25_DIFF', 'P_DF_LAST_3_DIFF', 'P_DF_LAST_50_DIFF',\n",
    "       'P_DF_LAST_5_DIFF', 'WIN_LAST_100_DIFF',\n",
    "       'WIN_LAST_10_DIFF', 'WIN_LAST_200_DIFF', 'WIN_LAST_25_DIFF',\n",
    "       'WIN_LAST_3_DIFF', 'WIN_LAST_50_DIFF', 'WIN_LAST_5_DIFF'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training vs Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll shuffle the data, and do a 85% split between training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "def build_temporal_data(df_subset):\n",
    "    return TemporalData(\n",
    "        src = torch.tensor(df_subset[\"p1_id\"].values, dtype=torch.long),\n",
    "        dst = torch.tensor(df_subset[\"p2_id\"].values, dtype=torch.long),\n",
    "        t   = torch.tensor(df_subset[\"t_days\"].values, dtype=torch.long),\n",
    "        msg = torch.tensor(df_subset[features_cols].values, dtype=torch.float),\n",
    "        y   = torch.tensor(df_subset[\"RESULT\"].values, dtype=torch.float),\n",
    "        closeness = torch.tensor(df_subset[\"score_closeness\"].values,dtype=torch.float)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the result column to string values (since that's what the sklearn library requires I'm pretty sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = build_temporal_data(final_dataset)\n",
    "train_data = build_temporal_data(train_df)\n",
    "test_data  = build_temporal_data(test_df)\n",
    "\n",
    "for data in (full_data,train_data, test_data):\n",
    "    data.src = data.src.long()    # src en ints longs\n",
    "    data.dst = data.dst.long()    # dst en ints longs\n",
    "    data.t   = data.t.long()     # timestamps en floats\n",
    "    data.msg = data.msg.float()   # features en floats\n",
    "    data.y   = data.y.float()     # labels en floats\n",
    "    data.closeness = data.closeness.float()\n",
    "# 4) D√©placer t et msg sur GPU\n",
    "device = \"cuda\"\n",
    "for data in (full_data,train_data, test_data):\n",
    "    data.t   = data.t.to(device)\n",
    "    data.msg = data.msg.to(device)\n",
    "# 6. DataLoaders pour entra√Ænement\n",
    "train_loader = TemporalDataLoader(train_data, batch_size=32, neg_sampling_ratio=0)\n",
    "test_loader  = TemporalDataLoader(test_data, batch_size=32, neg_sampling_ratio=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nouveau batch ===\n",
      "src: tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,  2,  6,\n",
      "        10, 12, 19, 22, 24, 29,  0, 27,  5, 22,  5, 32, 34, 36])\n",
      "dst: tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,  0,  5,\n",
      "         8, 15, 16, 21, 27, 30,  5, 29, 15, 29, 22, 33, 35, 37])\n",
      "t: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "msg: tensor([[ 0.9601,  0.5384, -0.5188,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-1.9789,  0.1350, -0.9501,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-0.7111, -0.1837,  1.0249,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        ...,\n",
      "        [ 0.6143,  1.0605, -0.3902,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-0.5575,  0.1548, -0.1783,  ..., -0.0022,  0.0043, -0.0023],\n",
      "        [-1.0377, -0.1670,  0.3741,  ..., -0.0022,  0.0043, -0.0023]],\n",
      "       device='cuda:0')\n",
      "y: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "Closeness: tensor([0.5882, 0.5882, 0.7826, 1.0000, 0.7368, 0.4000, 0.7619, 0.8966, 0.5882,\n",
      "        0.8462, 0.9655, 0.9333, 0.4000, 0.6667, 0.5000, 0.5882, 1.0000, 0.5000,\n",
      "        0.9375, 0.5000, 0.9697, 0.8000, 0.9375, 0.8387, 0.7619, 0.8966, 0.9286,\n",
      "        0.7692, 0.9231, 0.8571, 0.5000, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_loader:\n",
    "    print(\"=== Nouveau batch ===\")\n",
    "    print(\"src:\", batch.src)\n",
    "    print(\"dst:\", batch.dst)\n",
    "    print(\"t:\", batch.t)\n",
    "    print(\"msg:\", batch.msg)\n",
    "    print(\"y:\", batch.y)\n",
    "    print(\"Closeness:\", batch.closeness)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class History:\n",
    "    def __init__(self, outdir=\"runs/curves\", hparams=None):\n",
    "        \"\"\"\n",
    "        hparams: dict optionnel avec les hyperparam√®tres \n",
    "                 (ex: {\"lr\":4e-4,\"weight_decay\":1e-5,\"memory_dim\":128,...})\n",
    "        \"\"\"\n",
    "        self.outdir = Path(outdir)\n",
    "        self.outdir.mkdir(parents=True, exist_ok=True)\n",
    "        self.rows = []\n",
    "        self.prec_at_rows = []\n",
    "        self.detail_rows = []\n",
    "        self.hparams = hparams if hparams is not None else {}\n",
    "\n",
    "    def log_epoch(self, epoch,\n",
    "                  train_loss, train_ap, train_prec,\n",
    "                  val_loss,   val_ap,   val_prec,\n",
    "                  prec_at=None):\n",
    "        self.rows.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"train_ap\": float(train_ap),\n",
    "            \"train_prec@0.5\": float(train_prec),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_ap\": float(val_ap),\n",
    "            \"val_prec@0.5\": float(val_prec),\n",
    "        })\n",
    "        if prec_at is not None:\n",
    "            self.prec_at_rows.append(\n",
    "                {\"epoch\": epoch, **{f\"@{k}\": float(v) for k, v in prec_at.items()}}\n",
    "            )\n",
    "\n",
    "    def save_tables(self):\n",
    "        pd.DataFrame(self.rows).to_csv(self.outdir / \"metrics_history.csv\", index=False)\n",
    "        if self.prec_at_rows:\n",
    "            pd.DataFrame(self.prec_at_rows).to_csv(self.outdir / \"precision_at_history.csv\", index=False)\n",
    "        if self.detail_rows:\n",
    "            pd.DataFrame(self.detail_rows).to_csv(self.outdir / \"predicted_dates.csv\", index=False)\n",
    "        # üíæ Sauvegarde aussi les hyperparam√®tres dans un JSON\n",
    "        if self.hparams:\n",
    "            with open(self.outdir / \"hparams.json\",\"w\") as f:\n",
    "                json.dump(self.hparams,f,indent=2)\n",
    "\n",
    "    def _plot_and_save(self, x, y, ylabel, fname):\n",
    "        plt.figure()\n",
    "        plt.plot(x, y)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.outdir / fname, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    def save_plots(self):\n",
    "        df = pd.DataFrame(self.rows)\n",
    "        x = df[\"epoch\"].values\n",
    "        self._plot_and_save(x, df[\"train_loss\"].values, \"train_loss\", \"curve_train_loss.png\")\n",
    "        self._plot_and_save(x, df[\"val_loss\"].values,   \"val_loss\",   \"curve_val_loss.png\")\n",
    "        self._plot_and_save(x, df[\"train_ap\"].values,   \"train_AP\",   \"curve_train_ap.png\")\n",
    "        self._plot_and_save(x, df[\"val_ap\"].values,     \"val_AP\",     \"curve_val_ap.png\")\n",
    "        self._plot_and_save(x, df[\"train_prec@0.5\"].values, \"train_precision@0.5\", \"curve_train_prec.png\")\n",
    "        self._plot_and_save(x, df[\"val_prec@0.5\"].values,   \"val_precision@0.5\",   \"curve_val_prec.png\")\n",
    "\n",
    "    def save_all(self):\n",
    "        self.save_tables()\n",
    "        self.save_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN 1 | lr=0.001 | wd=0.0001 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:05<00:00, 43.93batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 104.57batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:06<00:00, 43.36batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 92.23batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:07<00:00, 42.57batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 102.38batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:11<00:00, 40.20batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 96.13batch/s] \n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:10<00:00, 40.77batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 100.72batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:13<00:00, 39.38batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.66batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:15<00:00, 38.21batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.16batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.60batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.57batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.87batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.88batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.38batch/s] \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 85.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run1_lr0.001_wd0.0001\n",
      "\n",
      "=== RUN 2 | lr=0.001 | wd=0.0001 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:13<00:00, 39.14batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 94.59batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:15<00:00, 38.15batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:03<00:00, 25.52batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:14<00:00, 38.61batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.96batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:14<00:00, 38.61batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 91.47batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:13<00:00, 39.36batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 97.05batch/s] \n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:13<00:00, 39.34batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.21batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 35.71batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 66.93batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.84batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.31batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:14<00:00, 38.94batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 73.04batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.26batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 81.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run2_lr0.001_wd0.0001\n",
      "\n",
      "=== RUN 3 | lr=0.001 | wd=0.0005 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:22<00:00, 34.92batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.47batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.23batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 74.35batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.70batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.55batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 35.94batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 81.62batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.35batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.14batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:15<00:00, 38.29batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.21batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.90batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 83.31batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.22batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.63batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.07batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.37batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run3_lr0.001_wd0.0005\n",
      "\n",
      "=== RUN 4 | lr=0.001 | wd=0.0005 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 36.10batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.02batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.79batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.50batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.46batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.52batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.12batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.94batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.08batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.60batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.50batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 93.90batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.88batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.37batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 35.98batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.83batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.75batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 89.27batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.55batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run4_lr0.001_wd0.0005\n",
      "\n",
      "=== RUN 5 | lr=0.0004 | wd=0.0001 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.43batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.95batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.26batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.36batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.13batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 93.26batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:14<00:00, 38.58batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.20batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:13<00:00, 39.38batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 95.79batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.21batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.65batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:23<00:00, 34.74batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.77batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.42batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.92batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.48batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 81.24batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 36.02batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 86.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run5_lr0.0004_wd0.0001\n",
      "\n",
      "=== RUN 6 | lr=0.0004 | wd=0.0001 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.07batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.80batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 85.89batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.95batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:04<00:00, 22.29batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.18batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 86.89batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.75batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 86.00batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.31batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.20batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:22<00:00, 34.87batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 88.37batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.83batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 87.46batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:18<00:00, 36.81batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.69batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:22<00:00, 34.86batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run6_lr0.0004_wd0.0001\n",
      "\n",
      "=== RUN 7 | lr=0.0004 | wd=0.0005 | hidden=[512, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 190,849\n",
      "Total parameters: 829,569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.17batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 83.43batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:23<00:00, 34.70batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 86.95batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.51batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.56batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:20<00:00, 35.68batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 84.87batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:24<00:00, 34.21batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 83.60batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:21<00:00, 35.35batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 85.03batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:19<00:00, 36.42batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 82.42batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:21<00:00, 35.37batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 82.40batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:17<00:00, 37.12batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 75.92batch/s]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:22<00:00, 35.18batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 83.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s 10 epochs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/tensorflow_project/tennis/env/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbes et CSV sauvegard√©s dans /home/romain/tensorflow_project/tennis/random-forest-tennis/random-forest-tennis/runs/run7_lr0.0004_wd0.0005\n",
      "\n",
      "=== RUN 8 | lr=0.0004 | wd=0.0005 | hidden=[256, 64] ===\n",
      "TGNMemory params: 464,192\n",
      "MultiLayerTimeAwareGNN params: 174,528\n",
      "SmallWinPredictor params: 108,161\n",
      "Total parameters: 746,881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2889/2889 [01:16<00:00, 37.93batch/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:01<00:00, 90.50batch/s]\n",
      "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2616/2889 [01:10<00:07, 35.33batch/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from torch_geometric.nn import TGNMemory\n",
    "from torch_geometric.nn.models.tgn import (\n",
    "    LastAggregator,\n",
    "    LastNeighborLoader,\n",
    "    IdentityMessage\n",
    ")\n",
    "from tgn.model import MultiLayerTimeAwareGNN,MessageMLP,WinPredictorMLP,WinPredictor,SmallWinPredictor\n",
    "from tgn.utils import train,evaluate,compute_alpha,train_debug,train_debug2\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Param√®tres\n",
    "memory_dim = 128\n",
    "time_dim   = 32\n",
    "embedding_dim = 128\n",
    "in_channels = 128\n",
    "hidden_channels = 32\n",
    "\n",
    "num_layers = 2\n",
    "heads = 4\n",
    "dropout= 0.4\n",
    "learning_rates = [1e-3, 4e-4]\n",
    "weight_decays = [1e-4, 5e-4]\n",
    "hidden_variants = [[512, 64], [256, 64]]\n",
    "\n",
    "# G√©n√©rer toutes les combinaisons\n",
    "grid = list(product(learning_rates, weight_decays, hidden_variants))\n",
    "run_id = 0\n",
    "for lr, wd, hidden in grid:\n",
    "    run_id += 1\n",
    "    print(f\"\\n=== RUN {run_id} | lr={lr} | wd={wd} | hidden={hidden} ===\")\n",
    "\n",
    "    msg_dim = full_data.msg.size(-1)\n",
    "\n",
    "    memory = TGNMemory(\n",
    "        num_nodes=num_nodes,\n",
    "        raw_msg_dim=msg_dim,\n",
    "        memory_dim=memory_dim,\n",
    "        time_dim=time_dim,\n",
    "        message_module=MessageMLP(msg_dim, memory_dim, time_dim,2*memory_dim),\n",
    "        aggregator_module=LastAggregator(),\n",
    "    ).to(device)\n",
    "\n",
    "    gnn = MultiLayerTimeAwareGNN(in_channels,memory_dim,hidden_channels, \n",
    "                                 embedding_dim, msg_dim, memory.time_enc,\n",
    "                                 num_layers,heads,dropout).to(device)\n",
    "    \n",
    "    win_pred = SmallWinPredictor(\n",
    "        embed_dim=embedding_dim,\n",
    "        match_dim=msg_dim,\n",
    "        hidden = hidden \n",
    "    ).to(device)\n",
    "\n",
    "    total_params = 0\n",
    "    for model in [memory, gnn, win_pred]:\n",
    "        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"{model.__class__.__name__} params: {model_params:,}\")\n",
    "        total_params += model_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(memory.parameters()) + list(gnn.parameters()) + list(win_pred.parameters()),\n",
    "        lr=lr,weight_decay=wd\n",
    "    )\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # === Loaders ===\n",
    "\n",
    "\n",
    "    train_loader_ngh = LastNeighborLoader(num_nodes=num_nodes, size=25, device=device)\n",
    "    eval_loader_ngh  = LastNeighborLoader(num_nodes=num_nodes, size=25, device=device)\n",
    "\n",
    "    assoc = torch.empty(num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\n",
    "    threshold = [0.6,0.65,0.7,0.75,0.8]\n",
    "    num_epochs = 200\n",
    "\n",
    "    import random\n",
    "\n",
    "    train_variants = [\n",
    "        (train_loader, full_data, train_data),\n",
    "\n",
    "    ]\n",
    "    patience = 10         # nombre d'epochs sans am√©lioration avant arr√™t\n",
    "    min_delta = 1e-4      # am√©lioration minimale pour reset la patience\n",
    "    best_val_loss = 100\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.9)\n",
    "    hparams = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": wd,\n",
    "    \"memory_dim\": memory_dim,\n",
    "    \"time_dim\": time_dim,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "    \"hidden\": hidden\n",
    "    }\n",
    "\n",
    "    run_dir = f\"runs/run{run_id}_lr{lr}_wd{wd}\"\n",
    "    os.makedirs(run_dir.replace(\"runs/\", \"models/\"), exist_ok=True)\n",
    "    history = History(outdir=run_dir,hparams=hparams)\n",
    "\n",
    "    train_losses, train_aps, train_prec = [], [], []\n",
    "    val_losses,   val_aps,   val_prec  = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        alpha = compute_alpha(epoch, num_epochs)\n",
    "\n",
    "        loader, full, train_data_split = random.choice(train_variants)\n",
    "\n",
    "        loss, ap, prec = train_debug2(\n",
    "            loader, memory, gnn, win_pred, full, train_loader_ngh, eval_loader_ngh,\n",
    "            optimizer, device, assoc, train_data_split, alpha\n",
    "        )\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        train_aps.append(ap)\n",
    "        train_prec.append(prec)\n",
    "\n",
    "        val_ap, val_loss, prec_v, prec_at, well_dates, bad_dates = evaluate(\n",
    "            test_loader, memory, gnn, win_pred, full_data, eval_loader_ngh,\n",
    "            assoc, device, threshold, alpha\n",
    "        )\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_aps.append(val_ap)\n",
    "        val_prec.append(prec_v)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping (aucune am√©lioration apr√®s {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "        # --- LOG + SAUVEGARDE INCR√âMENTALE  ---\n",
    "        history.log_epoch(\n",
    "            epoch=epoch,\n",
    "            train_loss=loss, train_ap=ap, train_prec=prec,\n",
    "            val_loss=val_loss, val_ap=val_ap, val_prec=prec_v,\n",
    "            prec_at=prec_at\n",
    "        )\n",
    "        \n",
    "        history.save_tables()\n",
    "        history.save_plots()\n",
    "        torch.save({\n",
    "        \"memory_state\": memory.state_dict(),\n",
    "        \"gnn_state\": gnn.state_dict(),\n",
    "        \"win_pred_state\": win_pred.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict()\n",
    "    }, f\"models/run{run_id}_lr{lr}_wd{wd}/epoch{epoch}.pth\")\n",
    "\n",
    "    \n",
    "    history.save_all()\n",
    "    print(\"Courbes et CSV sauvegard√©s dans\", history.outdir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
